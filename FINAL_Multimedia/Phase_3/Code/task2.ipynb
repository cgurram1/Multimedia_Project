{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import mds\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from accuracy_metrics import accuracy\n",
    "import torchvision.datasets\n",
    "from helpers import fetch_constant\n",
    "dataset_path = fetch_constant(\"caltech_dataset_path\")\n",
    "dataset = torchvision.datasets.Caltech101(root = dataset_path,download = False )\n",
    "%matplotlib widget\n",
    "from helpers import fetch_constant\n",
    "import pymongo\n",
    "cl = pymongo.MongoClient(fetch_constant('mongo_db_cs'))\n",
    "db = cl[\"caltech101db\"]\n",
    "collection = db[fetch_constant('phase2Trainingset_collection_name')]\n",
    "featurespace=\"resnet50_avgpool\"\n",
    "odd_collection = db[fetch_constant('odd_images_collection_name')]\n",
    "odd_dataset = list(collection.find({},{'_id':0,'label':1,f\"{featurespace}_feature_descriptor\":1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate pearson distance\n",
    "def pearson_correlation_distance(a,b):    \n",
    "    temp = pearsonr(a,b).statistic\n",
    "    return 1-temp\n",
    "# Fuction to calculate euclidian distance\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "#Creating a matrix which contains the distances from each point to every other point in the even dataset\n",
    "def create_distance_matrix(data,distance_type):\n",
    "    d_matrix = []\n",
    "    distance_mat = np.zeros(shape=(len(data),len(data)))\n",
    "    for idx1,p1 in enumerate(data):\n",
    "        distance = []\n",
    "        for idx2,p2 in enumerate(data):\n",
    "            if idx2<idx1:\n",
    "                distance.append(distance_mat[idx2][idx1])\n",
    "            else:\n",
    "                if distance_type == \"euclidian\":\n",
    "                    distance_mat[idx1][idx2]=euclidean_distance(p1,p2)\n",
    "                elif distance_type == \"pearson\":\n",
    "                    distance_mat[idx1][idx2]=pearson_correlation_distance(p1,p2)\n",
    "                distance.append(distance_mat[idx1][idx2])\n",
    "        d_matrix.append(distance)\n",
    "    return d_matrix\n",
    "# Function to find all the neighbors near passed point id within the distance of epsilon\n",
    "def region_query_v2(d_matrix,point_id,eps):\n",
    "    seeds = []\n",
    "    for i,dist in enumerate(d_matrix[point_id]):\n",
    "        if dist<eps:\n",
    "            seeds.append(i)\n",
    "    return seeds\n",
    "# Function to get a list of distances of kth nearest point of each point\n",
    "def calculate_kn_distance_v2(d_matrix,k):\n",
    "    kn_dist = []\n",
    "    for distances in d_matrix:\n",
    "        temp = sorted(distances)\n",
    "        kn_dist.append(temp[k])\n",
    "    return kn_dist\n",
    "# Function to look into neigbors of point and add them to cluster and expand them in case they are core points\n",
    "def expand_cluster_v2(d_matrix, classifications, point_id, cluster_id, eps, min_points):\n",
    "    seeds = region_query_v2(d_matrix, point_id, eps)\n",
    "    if len(seeds) < min_points:\n",
    "        classifications[point_id] = -1\n",
    "        return False\n",
    "    else:\n",
    "        classifications[point_id] = cluster_id\n",
    "        for seed_id in seeds:\n",
    "            classifications[seed_id] = cluster_id\n",
    "\n",
    "        while len(seeds) > 0:\n",
    "            current_point = seeds[0]\n",
    "            results = region_query_v2(d_matrix, current_point, eps)\n",
    "            if len(results) >= min_points:\n",
    "                for i in range(0, len(results)):\n",
    "                    result_point = results[i]\n",
    "                    if classifications[result_point] == 0 or classifications[result_point] == -1:\n",
    "                        if classifications[result_point] == 0:\n",
    "                            seeds.append(result_point)\n",
    "                        classifications[result_point] = cluster_id\n",
    "            seeds = seeds[1:]\n",
    "        return True\n",
    "\n",
    "def dbscan_v2(d_matrix, eps, min_points):\n",
    "    cluster_id = 1\n",
    "    n_points = d_matrix.shape[0]\n",
    "    classifications = [0]*n_points\n",
    "    for point_id in range(0, n_points):\n",
    "        point = d_matrix[point_id,:]\n",
    "        if classifications[point_id] == 0:\n",
    "            if expand_cluster_v2(d_matrix, classifications, point_id, cluster_id, eps, min_points):\n",
    "                cluster_id = cluster_id + 1\n",
    "    return classifications\n",
    "# Function to generate combinations of minpts and epsilons to try out and get required c number of clusters\n",
    "def create_combinations2(min,mean,minPts,divisions):\n",
    "    a_arr=[]\n",
    "    decrement = (mean-min)/divisions\n",
    "    j= decrement\n",
    "    while mean-j>min:\n",
    "        a_arr.append(mean-j)\n",
    "        j+=decrement\n",
    "    result = []\n",
    "    for i in a_arr:\n",
    "        result.append((i,minPts))\n",
    "    return result   \n",
    "# Function to count the number of points within each cluster\n",
    "def cluster_count(clusters):\n",
    "    temp = dict()\n",
    "    for k in clusters:\n",
    "        if k in temp.keys():\n",
    "            temp[k]+=1\n",
    "        else:\n",
    "            temp[k] = 1\n",
    "    return temp\n",
    "# Function to find all the core points of the clustering for given label\n",
    "core_pts = []\n",
    "def get_core_points(label,d_matrix,label_dataset,eps,minPts):\n",
    "    for point_id in range(len(d_matrix)):\n",
    "        seeds = region_query_v2(d_matrix, point_id, eps)\n",
    "        if len(seeds) >=minPts:\n",
    "            core_pts.append((label,label_dataset[point_id]))\n",
    "# Function to extract the distances between points belonging to the same label from overall distance matrix\n",
    "def get_label_distance_matrix(d_matrix,lable_image_ids):\n",
    "    label_distance_matrix = np.zeros(shape=(len(lable_image_ids),len(lable_image_ids)))\n",
    "    for idx1,ele1 in enumerate(lable_image_ids):\n",
    "        for idx2,ele2 in enumerate(lable_image_ids):\n",
    "            label_distance_matrix[idx1][idx2] = d_matrix[ele1//2][ele2//2]\n",
    "    return label_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionarys that has keys as label number and values as the feature descriptors \n",
    "# of images belonging to that labels and their corresponding image ids\n",
    "label_dataset = defaultdict(list)\n",
    "label_image_ids = defaultdict(list)\n",
    "even_dataset =list(collection.find({},{'_id':0,\"image_id\":1, 'label': 1,f\"{featurespace}_feature_descriptor\":1}))\n",
    "even_images = []\n",
    "for i in range(len(even_dataset)):\n",
    "    even_images.append(np.array(even_dataset[i][f\"{featurespace}_feature_descriptor\"]).flatten())\n",
    "    label_dataset[even_dataset[i]['label']].append(np.array(even_dataset[i][f\"{featurespace}_feature_descriptor\"]).flatten())\n",
    "    label_image_ids[even_dataset[i]['label']].append(even_dataset[i]['image_id'])\n",
    "d_matrix = create_distance_matrix(even_images,\"euclidian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_end = 0\n",
    "major_cluster_indexing = []\n",
    "major_cluster_pts = []\n",
    "chosen_cluster_parameters = defaultdict(lambda:0)\n",
    "for label_num in range(101): \n",
    "    print(f\"class: {label_num}\",end=\"\\r\")   \n",
    "    c=5\n",
    "    minPts = 10\n",
    "    max_diff = 1e9\n",
    "    overall_number_of_outliers = 1e9\n",
    "    chosen_clusters = None\n",
    "    cluster_count_dict = {}\n",
    "    label_dist_mat = get_label_distance_matrix(d_matrix,label_image_ids[label_num])\n",
    "    for k in range(minPts,1,-1):\n",
    "        # getting the distances between each point and its K'th neighbor( where k is a minPts value)\n",
    "        d = calculate_kn_distance_v2(label_dist_mat,k)\n",
    "        # generating combinations using k as Minpts and epsilon values\n",
    "        combi = create_combinations2(min(d),np.mean(d),k,100)\n",
    "        # Iterating through all generated combinations,for each combination generated, applying dbscan on data.\n",
    "        for i,j in combi:\n",
    "            clusters = dbscan_v2(label_dist_mat,i,j)\n",
    "            temp = cluster_count(clusters)\n",
    "            current_no_of_outliers=1e10\n",
    "            # counting number of valid clusters generated by dbscan for current combination of minpts and epsilon\n",
    "            count = 0\n",
    "            for i in temp:\n",
    "                if i!=-1:\n",
    "                    count+=1\n",
    "            # getting number of points classified as noise by dbscan for current combination of minpts and epsilon   \n",
    "            if( -1 in temp):\n",
    "                current_no_of_outliers=temp[-1]\n",
    "            else:\n",
    "                current_no_of_outliers=0\n",
    "            # for first occurance where number of clusters generated = required number of links \n",
    "            if count==c and max_diff!=0:\n",
    "                max_diff = 0\n",
    "                chosen_clusters = clusters\n",
    "                chosen_cluster_parameters[label_num] = (i,j)\n",
    "                # recording number of points as noise in current chosen cluster\n",
    "                overall_number_of_outliers=current_no_of_outliers\n",
    "            # for occurance of number of clusters generated = c and if noise in current cluster is lesser than previous chosen clusters\n",
    "            if count==c and max_diff==0 and current_no_of_outliers<overall_number_of_outliers:\n",
    "                chosen_clusters = clusters\n",
    "                chosen_cluster_parameters[label_num] = (i,j)\n",
    "                overall_number_of_outliers=current_no_of_outliers\n",
    "\n",
    "            if (abs(c-count)==max_diff and current_no_of_outliers<overall_number_of_outliers) or (abs(c-count)<max_diff):\n",
    "                chosen_clusters = clusters\n",
    "                chosen_cluster_parameters[label_num] = (i,j)\n",
    "                max_diff = abs(c-count)\n",
    "                overall_number_of_outliers=current_no_of_outliers\n",
    "    \n",
    "    get_core_points(label_num,label_dist_mat,label_dataset[label_num],chosen_cluster_parameters[label_num][0],chosen_cluster_parameters[label_num][1])\n",
    "    # finding cluster with maximum number of points and using the major cluster as a representative of the label\n",
    "    major_cluster = None\n",
    "    var1 = cluster_count(chosen_clusters)\n",
    "    for i,j in var1.items():     \n",
    "        if j==max(var1.values()):\n",
    "            major_cluster = i\n",
    "    # getting the points that belong to chosen_cluster using the indices of the list corresponding to their cluster ids\n",
    "    chosen_cluster_points = []\n",
    "    for index,cluster in enumerate(chosen_clusters):\n",
    "        if cluster ==major_cluster:\n",
    "            chosen_cluster_points.append(label_dataset[label_num][index])\n",
    "    # creating an indexing for noting the points of the major clusters that represent each label\n",
    "    major_cluster_indexing.append((prev_end,prev_end+len(chosen_cluster_points)-1))\n",
    "    prev_end +=len(chosen_cluster_points) \n",
    "    # adding major cluster points of each label to one big list that contains all major cluster points of all label\n",
    "    major_cluster_pts+=chosen_cluster_points\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    xybox=(50., 50.)\n",
    "    im = OffsetImage(dataset[0][0],zoom=.25)\n",
    "    ab = AnnotationBbox(im, (0,0), xybox=xybox, xycoords='data',\n",
    "            boxcoords=\"offset points\",  pad=0.3,  arrowprops=dict(arrowstyle=\"->\"))\n",
    "    # add it to the axes and make it invisible\n",
    "    ax.add_artist(ab)\n",
    "    ab.set_visible(False)\n",
    "\n",
    "    chosen_cluster_indices = defaultdict(list)\n",
    "    original_images = []\n",
    "    reduced_data=mds.mds(np.array(label_dataset[label_num]),2)\n",
    "    for i,c in enumerate(chosen_clusters):\n",
    "        chosen_cluster_indices[c].append((i,reduced_data[i]))\n",
    "        original_images.append(dataset[label_image_ids[label_num][i]][0])\n",
    "    \n",
    "    overall_x,overall_y,overall_indices = [],[],[]  \n",
    "    for i,clusters in chosen_cluster_indices.items():      \n",
    "        x,y,indices = [],[],[]\n",
    "        for j,pt in clusters:\n",
    "            x.append(pt[0])\n",
    "            overall_x.append(pt[0])\n",
    "            y.append(pt[1])\n",
    "            overall_y.append(pt[1])\n",
    "            indices.append(j)\n",
    "            overall_indices.append(j)\n",
    "        plt.scatter(x,y,marker=\".\",label = f\"Cluster:{i}\", zorder=1)\n",
    "\n",
    "    line, = ax.plot(overall_x,overall_y, ls=\"\", marker=\".\", zorder=0)\n",
    "    def hover(event, him=im,hab = ab,hline=line, hoverall_x = overall_x,hoverall_y = overall_y,horiginal_images = original_images,hoverall_indices = overall_indices,hfig = fig):\n",
    "        if hline.contains(event)[0]:\n",
    "            try:\n",
    "                # find out the index within the array from the event\n",
    "                ind, = hline.contains(event)[1][\"ind\"]\n",
    "                w,h = hfig.get_size_inches()*hfig.dpi\n",
    "                ws = (event.x > w/2.)*-1 + (event.x <= w/2.) \n",
    "                hs = (event.y > h/2.)*-1 + (event.y <= h/2.)\n",
    "                hab.xybox = (xybox[0]*ws, xybox[1]*hs)\n",
    "                hab.set_visible(True)\n",
    "                # place it at the position of the hovered scatter point\n",
    "                hab.xy =(hoverall_x[ind], hoverall_y[ind])\n",
    "                # set the image corresponding to that point\n",
    "                him.set_data(horiginal_images[hoverall_indices[ind]])\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            hab.set_visible(False)\n",
    "        hfig.canvas.draw_idle()\n",
    "\n",
    "    plt.legend()\n",
    "    fig.canvas.mpl_connect('motion_notify_event', hover)        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the labels of input images using the points that belongs to the major cluster of each label\n",
    "def predict_label(pt,cluster_pts_lst,cluster_indexing):\n",
    "    minPts = 51\n",
    "    kn_distance = []\n",
    "    for i in range(len(cluster_pts_lst)):\n",
    "        kn_distance.append((i,euclidean_distance(cluster_pts_lst[i],pt)))\n",
    "    kn_distance = sorted(kn_distance,key=lambda a:a[1])\n",
    "    poll = defaultdict(lambda: 0)\n",
    "    for i,_ in kn_distance[:minPts]:\n",
    "        for label,ele in enumerate(cluster_indexing):\n",
    "            if i>=ele[0] and i<=ele[1]:\n",
    "                poll[label] +=1\n",
    "                break\n",
    "    predicted_label = sorted(poll.items(),key = lambda a:a[1], reverse=True)[0][0]\n",
    "    return predicted_label\n",
    "\n",
    "#prediction of odd images\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "for dictionary in odd_dataset:\n",
    "    predicted_labels.append(predict_label(dictionary[f\"{featurespace}_feature_descriptor\"],major_cluster_pts,major_cluster_indexing))\n",
    "    actual_labels.append(dictionary['label'])\n",
    "\n",
    "accuracy.calculate_labelwise_metrics(accuracy.get_OneHot(actual_labels), accuracy.get_OneHot(predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
